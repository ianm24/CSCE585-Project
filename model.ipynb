{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_folder = \"./data/\"\n",
    "\n",
    "#Get and load data\n",
    "# pitch_14_17_file = \"pitcher_2014_2017.csv\"\n",
    "bball_data_2_file = \"Baseball Data-2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchingDataset(Dataset): #TODO set this up to work for our dataset\n",
    "#TODO modify so this so it will return a tuple [train_set, test_set, validation_set]\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    #defining prepprocessing functions\n",
    "\n",
    "    #preprocessing function used to calculate the plate location and isStrike classifier (this will be different than PitchCall (even in terms of BallCalled vs StrikeCalled))\n",
    "    #returns tuple: classification, isStrike\n",
    "    #classification: heart (strike) = 0, shadow (strike) = 1, shadow (ball) = 2, chase (ball) = 3, waste (ball) = 4\n",
    "    def PlateZone(self, PlateLocHeight, PlateLocSide):\n",
    "      foot = 12\n",
    "\n",
    "      #waste outside 84in to 6in, -20in to 20in horizontal, (strike zone * 200%)\n",
    "      if ((PlateLocHeight > 7 or PlateLocHeight < 0.5) and (PlateLocSide < -(20/foot) or PlateLocSide > (20/foot))):\n",
    "        return 4\n",
    "\n",
    "      #heart inside 38in to 22in vertical, -6.7in to 6.7in horizontal, (strike zone size * 67%)\n",
    "      if ((PlateLocHeight < (38/foot) and PlateLocHeight > (22/foot)) and (PlateLocSide > (-6.7/foot) and PlateLocSide < (6.7/foot))):\n",
    "        return 0\n",
    "\n",
    "      #strike zone inside 42in to 18in vertical, -10in to 10in horizontal\n",
    "      if (PlateLocHeight < (42/foot) and PlateLocHeight > (18/foot) and (PlateLocSide > (-10/foot) and PlateLocSide < (10/foot))):\n",
    "        return 1\n",
    "\n",
    "      #shadow inside 46in to 14in vertical, -13.3in to 13.3in horizontal, (strike zone size * 133%)\n",
    "      if (PlateLocHeight < (46/foot) and PlateLocHeight > (14/foot) and (PlateLocSide > (-13.3/foot) and PlateLocSide < (13.3/foot))):\n",
    "        return 2\n",
    "\n",
    "      #chase inside 84in to 6in, -20in to 20in horizontal, (strike zone * 200%)\n",
    "      return 3\n",
    "\n",
    "    #preprocessing function used to generate a single number that will be used to classify the ball/strike count before the current pitch \n",
    "    #returns int [0 - 11]\n",
    "    def PitchCount(self, balls, strikes):\n",
    "    # Strikes: 0  1   2\n",
    "    # Balls v|---------- \n",
    "    #       0| 0  1   2 \n",
    "    #       1| 3  4   5\n",
    "    #       2| 6  7   8\n",
    "    #       3| 9  10  11\n",
    "      if(balls == 0):\n",
    "        if(strikes == 0):\n",
    "          return 0\n",
    "        if (strikes == 1):\n",
    "          return 1\n",
    "        return 2\n",
    "      if(balls == 1):\n",
    "        if(strikes == 0):\n",
    "          return 3\n",
    "        if (strikes == 1):\n",
    "          return 4\n",
    "        return 5\n",
    "      if(balls == 2):\n",
    "        if(strikes == 0):\n",
    "          return 6\n",
    "        if (strikes == 1):\n",
    "          return 7\n",
    "        return 8\n",
    "      if(balls == 3):\n",
    "        if(strikes == 0):\n",
    "          return 9\n",
    "        if (strikes == 1):\n",
    "          return 10\n",
    "        return 11\n",
    "\n",
    "    #this preprocessing function generates the ground truth hitability of a pitch\n",
    "    #these values will definitely need to be adjusted and should probably be hyperparameters\n",
    "    def GenerateGroundTruthLabels(self, pitchCall):\n",
    "      if pitchCall == 'BallCalled':\n",
    "        return -1\n",
    "      if pitchCall == 'BallIntentional' or pitchCall == 'HitByPitch':\n",
    "        return 0\n",
    "      if pitchCall == 'StrikeSwinging' or pitchCall == 'StrikeCalled':\n",
    "        return 1\n",
    "      else:\n",
    "        return 2\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, train, validation):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with data.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #import the columns we will need for training and preprocessing\n",
    "        self.root_dir = root_dir\n",
    "        self.bball_data = pd.read_csv(self.root_dir+csv_file, usecols=['Pitcher', 'PitcherThrows', 'Batter', 'BatterSide', 'PitchCall', 'KorBB', 'PlayResult', 'RunsScored', 'VertBreak', 'HorzBreak', 'ZoneSpeed', 'VertApprAngle', 'HorzApprAngle', 'ZoneTime', 'PlateLocHeight', 'PlateLocSide', 'Balls', 'Strikes', 'TaggedPitchType'])\n",
    "        #run preprocessing functions\n",
    "        self.bball_data['Zone'] = self.bball_data.apply(lambda pitch : self.PlateZone(pitch['PlateLocHeight'], pitch['PlateLocSide']), axis=1)\n",
    "        self.bball_data['BallStrikeNum'] = self.bball_data.apply(lambda pitch : self.PitchCount(pitch['Balls'], pitch['Strikes']), axis=1)\n",
    "        self.bball_data['GroundTruth'] = self.bball_data.apply(lambda pitch : self.GenerateGroundTruthLabels(pitch['PitchCall']), axis=1)\n",
    "\n",
    "        #drop all features we no longer need (ones only used for preprocessing)\n",
    "        self.bball_data.drop(labels=['PlateLocHeight', 'PlateLocSide', 'Balls', 'Strikes'], axis=1)\n",
    "\n",
    "\n",
    "        categorical_columns = ['Pitcher', 'Batter', 'PitchCall', 'KorBB', 'PlayResult', 'Zone', 'BallStrikeNum']\n",
    "\n",
    "        for category in categorical_columns:\n",
    "          self.bball_data[category] = self.bball_data[category].astype('category')\n",
    "\n",
    "        #print the labels of the features we will use to train our network\n",
    "        print(self.bball_data.keys())\n",
    "\n",
    "        #splitting data into training, validation, testing\n",
    "        total_samples = len(self.bball_data.index)\n",
    "        training_samples = math.floor(0.6*total_samples)\n",
    "        validation_samples = math.ceil(0.2*total_samples)\n",
    "        testing_samples = math.ceil(0.4*total_samples) #TODO change this back to 0.2 once validation implemented\n",
    "\n",
    "        sum = training_samples+testing_samples#+validation_samples\n",
    "\n",
    "        print(\"total samples:\",total_samples,\n",
    "              \"\\ntraining samples:\",training_samples,\n",
    "              #\"\\nvalidation samples:\",validation_samples,\n",
    "              \"\\ntesting samples:\",testing_samples,\n",
    "              \"\\nsum of training, validation, and test samples:\",sum)\n",
    "\n",
    "        #makes shuffled version of the data\n",
    "        indices = np.arange(total_samples)\n",
    "        np.random.shuffle(indices) #TODO get ground truth for data sets using these indices\n",
    "        shuffled_bball_data = self.bball_data.reindex(indices).reset_index()\n",
    "\n",
    "        #gets the amount of random data points as determined by set proportion\n",
    "        if train:\n",
    "          training_data = shuffled_bball_data.iloc[0:training_samples]\n",
    "          self.bball_data = training_data\n",
    "          print(\"training\")\n",
    "        elif validation:\n",
    "          validation_data = shuffled_bball_data.iloc[training_samples:training_samples+validation_samples]\n",
    "          self.bball_data = validation_data\n",
    "          print(\"validation\")\n",
    "        else:\n",
    "          testing_data = shuffled_bball_data.iloc[training_samples+validation_samples:training_samples+validation_samples+testing_samples]\n",
    "          self.bball_data = testing_data\n",
    "          print(\"testing\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bball_data)\n",
    "\n",
    "    def __getitem__(self, idx): #TODO make this get the next data point\n",
    "       # if torch.is_tensor(idx):\n",
    "        #    idx = idx.tolist()\n",
    "        #taken from this https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "        # img_name = os.path.join(self.root_dir,\n",
    "        #                         self.landmarks_frame.iloc[idx, 0])\n",
    "        # image = io.imread(img_name)\n",
    "        # landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        # landmarks = np.array([landmarks])\n",
    "        # landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "\n",
    "        num_data = len(self.bball_data)\n",
    "        num_features = len(self.bball_data.columns) - 2\n",
    "        ground_truth_index = num_features + 1\n",
    "\n",
    "        #print(num_data)\n",
    "        #print(num_features)\n",
    "        #print(ground_truth_index)\n",
    "\n",
    "        all_data = self.bball_data.iloc[[idx]]\n",
    "\n",
    "        #print(all_data)\n",
    "        #print(all_data.shape)\n",
    "\n",
    "        ground_truth = all_data.pop('GroundTruth')\n",
    "        predictive_data = all_data\n",
    "\n",
    "        #print(ground_truth)\n",
    "        #print(predictive_data)\n",
    "\n",
    "        sample = {'predictive_data': predictive_data, 'ground_truth': ground_truth}\n",
    "\n",
    "        #print(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    #will return a dataframe containing all rows which have the specified key value in the feature column\n",
    "    def getAllWithFeature(self, feature, key):\n",
    "        possible_features = ['Pitcher', 'PitcherThrows', 'Batter', 'BatterSide', 'TaggedPitchType']\n",
    "\n",
    "        if feature not in possible_features:\n",
    "          return None\n",
    "        \n",
    "        pitches = self.bball_data.loc[self.bball_data[feature] == key]\n",
    "\n",
    "        return pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataBaseball Data-2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IANMCD~1\\AppData\\Local\\Temp/ipykernel_2104/4215668192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPitchingDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbball_data_2_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPitchingDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbball_data_2_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mballsCalled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAllWithFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BatterSide'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mballsCalled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\IANMCD~1\\AppData\\Local\\Temp/ipykernel_2104/1991224915.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, csv_file, root_dir, train, validation)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m#import the columns we will need for training and preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbball_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Pitcher'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PitcherThrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Batter'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BatterSide'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PitchCall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'KorBB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PlayResult'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RunsScored'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VertBreak'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HorzBreak'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ZoneSpeed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VertApprAngle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HorzApprAngle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ZoneTime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PlateLocHeight'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PlateLocSide'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Balls'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Strikes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TaggedPitchType'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;31m#run preprocessing functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbball_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Zone'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbball_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mpitch\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPlateZone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpitch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PlateLocHeight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PlateLocSide'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataBaseball Data-2.csv'"
     ]
    }
   ],
   "source": [
    "test_data = PitchingDataset(bball_data_2_file,data_folder,train=False,validation=False)\n",
    "train_data = PitchingDataset(bball_data_2_file,data_folder,train=True,validation=False)\n",
    "ballsCalled = test_data.getAllWithFeature('BatterSide', 'Right')\n",
    "\n",
    "print(len(ballsCalled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an implementation of the soft decision tree model\n",
    "#https://github.com/kimhc6028/soft-decision-tree/blob/master/model.py\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class InnerNode():\n",
    "\n",
    "    def __init__(self, depth, args):\n",
    "        self.args = args\n",
    "        self.fc = nn.Linear(self.args.input_dim, 1)\n",
    "        beta = torch.randn(1)\n",
    "        #beta = beta.expand((self.args.batch_size, 1))\n",
    "        if self.args.cuda:\n",
    "            beta = beta.cuda()\n",
    "        self.beta = nn.Parameter(beta)\n",
    "        self.leaf = False\n",
    "        self.prob = None\n",
    "        self.leaf_accumulator = []\n",
    "        self.lmbda = self.args.lmbda * 2 ** (-depth)\n",
    "        self.build_child(depth)\n",
    "        self.penalties = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.leaf_accumulator = []\n",
    "        self.penalties = []\n",
    "        self.left.reset()\n",
    "        self.right.reset()\n",
    "\n",
    "    def build_child(self, depth):\n",
    "        if depth < self.args.max_depth:\n",
    "            self.left = InnerNode(depth+1, self.args)\n",
    "            self.right = InnerNode(depth+1, self.args)\n",
    "        else :\n",
    "            self.left = LeafNode(self.args)\n",
    "            self.right = LeafNode(self.args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(F.sigmoid(self.beta*self.fc(x)))\n",
    "    \n",
    "    def select_next(self, x):\n",
    "        prob = self.forward(x)\n",
    "        if prob < 0.5:\n",
    "            return(self.left, prob)\n",
    "        else:\n",
    "            return(self.right, prob)\n",
    "\n",
    "    def cal_prob(self, x, path_prob):\n",
    "        self.prob = self.forward(x) #probability of selecting right node\n",
    "        self.path_prob = path_prob\n",
    "        left_leaf_accumulator = self.left.cal_prob(x, path_prob * (1-self.prob))\n",
    "        right_leaf_accumulator = self.right.cal_prob(x, path_prob * self.prob)\n",
    "        self.leaf_accumulator.extend(left_leaf_accumulator)\n",
    "        self.leaf_accumulator.extend(right_leaf_accumulator)\n",
    "        return(self.leaf_accumulator)\n",
    "\n",
    "    def get_penalty(self):\n",
    "        penalty = (torch.sum(self.prob * self.path_prob) / torch.sum(self.path_prob), self.lmbda)\n",
    "        if not self.left.leaf:\n",
    "            left_penalty = self.left.get_penalty()\n",
    "            right_penalty = self.right.get_penalty()\n",
    "            self.penalties.append(penalty)\n",
    "            self.penalties.extend(left_penalty)\n",
    "            self.penalties.extend(right_penalty)\n",
    "        return(self.penalties)\n",
    "\n",
    "\n",
    "class LeafNode():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.param = torch.randn(self.args.output_dim)\n",
    "        if self.args.cuda:\n",
    "            self.param = self.param.cuda()\n",
    "        self.param = nn.Parameter(self.param)\n",
    "        self.leaf = True\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self):\n",
    "        return(self.softmax(self.param.view(1,-1)))\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def cal_prob(self, x, path_prob):\n",
    "        Q = self.forward()\n",
    "        #Q = Q.expand((self.args.batch_size, self.args.output_dim))\n",
    "        Q = Q.expand((path_prob.size()[0], self.args.output_dim))\n",
    "        return([[path_prob, Q]])\n",
    "\n",
    "\n",
    "class SoftDecisionTree(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(SoftDecisionTree, self).__init__()\n",
    "        self.args = args\n",
    "        self.root = InnerNode(1, self.args)\n",
    "        self.collect_parameters() ##collect parameters and modules under root node\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=self.args.lr, momentum=self.args.momentum)\n",
    "        self.test_acc = []\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        self.best_accuracy = 0.0\n",
    "\n",
    "    def define_extras(self, batch_size):\n",
    "        ##define target_onehot and path_prob_init batch size, because these need to be defined according to batch size, which can be differ\n",
    "        self.target_onehot = torch.FloatTensor(batch_size, self.args.output_dim)\n",
    "        self.target_onehot = Variable(self.target_onehot)\n",
    "        self.path_prob_init = Variable(torch.ones(batch_size, 1))\n",
    "        if self.args.cuda:\n",
    "            self.target_onehot = self.target_onehot.cuda()\n",
    "            self.path_prob_init = self.path_prob_init.cuda()\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        node = self.root\n",
    "        path_prob = Variable(torch.ones(self.args.batch_size, 1))\n",
    "        while not node.leaf:\n",
    "            node, prob = node.select_next(x)\n",
    "            path_prob *= prob\n",
    "        return node()\n",
    "    '''        \n",
    "    def cal_loss(self, x, y):\n",
    "        batch_size = y.size()[0]\n",
    "        leaf_accumulator = self.root.cal_prob(x, self.path_prob_init)\n",
    "        loss = 0.\n",
    "        max_prob = [-1. for _ in range(batch_size)]\n",
    "        max_Q = [torch.zeros(self.args.output_dim) for _ in range(batch_size)]\n",
    "        for (path_prob, Q) in leaf_accumulator:\n",
    "            TQ = torch.bmm(y.view(batch_size, 1, self.args.output_dim), torch.log(Q).view(batch_size, self.args.output_dim, 1)).view(-1,1)\n",
    "            loss += path_prob * TQ\n",
    "            path_prob_numpy = path_prob.cpu().data.numpy().reshape(-1)\n",
    "            for i in range(batch_size):\n",
    "                if max_prob[i] < path_prob_numpy[i]:\n",
    "                    max_prob[i] = path_prob_numpy[i]\n",
    "                    max_Q[i] = Q[i]\n",
    "        loss = loss.mean()\n",
    "        penalties = self.root.get_penalty()\n",
    "        C = 0.\n",
    "        for (penalty, lmbda) in penalties:\n",
    "            C -= lmbda * 0.5 *(torch.log(penalty) + torch.log(1-penalty))\n",
    "        output = torch.stack(max_Q)\n",
    "        self.root.reset() ##reset all stacked calculation\n",
    "        return(-loss + C, output) ## -log(loss) will always output non, because loss is always below zero. I suspect this is the mistake of the paper?\n",
    "\n",
    "    def collect_parameters(self):\n",
    "        nodes = [self.root]\n",
    "        self.module_list = nn.ModuleList()\n",
    "        self.param_list = nn.ParameterList()\n",
    "        while nodes:\n",
    "            node = nodes.pop(0)\n",
    "            if node.leaf:\n",
    "                param = node.param\n",
    "                self.param_list.append(param)\n",
    "            else:\n",
    "                fc = node.fc\n",
    "                beta = node.beta\n",
    "                nodes.append(node.right)\n",
    "                nodes.append(node.left)\n",
    "                self.param_list.append(beta)\n",
    "                self.module_list.append(fc)\n",
    "\n",
    "    def train_(self, train_loader, epoch):\n",
    "        self.train()\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            correct = 0\n",
    "            if self.args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            #data = data.view(self.args.batch_size,-1)\n",
    "            target = Variable(target)\n",
    "            target_ = target.view(-1,1)\n",
    "            batch_size = target_.size()[0]\n",
    "            data = data.view(batch_size,-1)\n",
    "            ##convert int target to one-hot vector\n",
    "            data = Variable(data)\n",
    "            if not batch_size == self.args.batch_size: #because we have to initialize parameters for batch_size, tensor not matches with batch size cannot be trained\n",
    "                self.define_extras(batch_size)\n",
    "            self.target_onehot.data.zero_()            \n",
    "            self.target_onehot.scatter_(1, target_, 1.)\n",
    "            self.optimizer.zero_grad()\n",
    "            print(data.size())\n",
    "            loss, output = self.cal_loss(data, self.target_onehot)\n",
    "            #loss.backward(retain_variables=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "            accuracy = 100. * correct / len(data)\n",
    "\n",
    "            if batch_idx % self.args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.4f}%)'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(),\n",
    "                    correct, len(data),\n",
    "                    accuracy))\n",
    "\n",
    "    def test_(self, test_loader, epoch):\n",
    "        self.eval()\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            if self.args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            target = Variable(target)\n",
    "            target_ = target.view(-1,1)\n",
    "            batch_size = target_.size()[0]\n",
    "            data = data.view(batch_size,-1)\n",
    "            ##convert int target to one-hot vector\n",
    "            data = Variable(data)\n",
    "            if not batch_size == self.args.batch_size: #because we have to initialize parameters for batch_size, tensor not matches with batch size cannot be trained\n",
    "                self.define_extras(batch_size)\n",
    "            self.target_onehot.data.zero_()            \n",
    "            self.target_onehot.scatter_(1, target_, 1.)\n",
    "            _, output = self.cal_loss(data, self.target_onehot)\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print('\\nTest set: Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "            correct, len(test_loader.dataset),\n",
    "            accuracy))\n",
    "        self.test_acc.append(accuracy)\n",
    "\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.save_best('./result')\n",
    "            self.best_accuracy = accuracy\n",
    "\n",
    "    def save_best(self, path):\n",
    "        try:\n",
    "            os.makedirs('./result')\n",
    "        except:\n",
    "            print('directory ./result already exists')\n",
    "\n",
    "        with open(os.path.join(path, 'best_model.pkl'), 'wb') as output_file:\n",
    "            pickle.dump(self, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is driver code used to train the soft decision tree model\n",
    "#https://github.com/kimhc6028/soft-decision-tree/blob/master/main.py\n",
    "from __future__ import print_function\n",
    "import sys,os\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "sys.argv = ['']\n",
    "\n",
    "#training_data = <-- define to get rid of name error\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--input-dim', type=int, default=len(training_data.columns), metavar='N', #set to number of columns we are using\n",
    "                    help='input dimension size(default: 28 * 28)')\n",
    "parser.add_argument('--output-dim', type=int, default=10, metavar='N',\n",
    "                    help='output dimension size(default: 10)')\n",
    "parser.add_argument('--max-depth', type=int, default=8, metavar='N',\n",
    "                    help='maximum depth of tree(default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "                    help='number of epochs to train (default: 40)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lmbda', type=float, default=0.1, metavar='LR',\n",
    "                    help='temperature rate (default: 0.1)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# try:\n",
    "#     os.makedirs('./data')\n",
    "# except:\n",
    "#     print('directory ./data already exists')\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    PitchingDataset(bball_data_2_file,data_folder,train=True,validation=False),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# validation_loader = torch.utils.data.DataLoader(\n",
    "#     PitchingDataset(bball_data_2_file,data_folder,train=False,validation=True),\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    PitchingDataset(bball_data_2_file,data_folder,train=False,validation=False),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('./data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#     ])),\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "def save_result(acc):\n",
    "    try:\n",
    "        os.makedirs('./result')\n",
    "    except:\n",
    "        print('directory ./result already exists')\n",
    "    filename = os.path.join('./result/', 'bp.pickle')\n",
    "    f = open(filename,'wb')\n",
    "    pickle.dump(acc, f)\n",
    "    f.close()\n",
    "\n",
    "model = SoftDecisionTree(args)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model.train_(train_loader, epoch)\n",
    "    model.test_(test_loader, epoch)\n",
    "\n",
    "save_result(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44b4c03e958481dbaa2eafea0c017cfed0f1c42c683a29dbaa342c2b25b9bb05"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
