{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_folder = \"./data/\"\n",
    "\n",
    "#Get and load data\n",
    "# pitch_14_17_file = \"pitcher_2014_2017.csv\"\n",
    "bball_data_2_file = \"Baseball Data-2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contains variables training_data, testing_data, and validation_data which are randomized samples of preset proportions of the dataset\n",
    "#dataset contains the features:\n",
    "#['Pitcher', 'PitcherThrows', 'Batter', 'BatterSide', 'Balls', 'Strikes', 'TaggedPitchType', 'PitchCall', 'KorBB', 'PlayResult', 'RunsScored',\n",
    "#'VertBreak', 'HorzBreak', 'PlateLocHeight', 'PlateLocSide', 'ZoneSpeed', 'VertApprAngle', 'HorzApprAngle', 'ZoneTime', 'Zone', 'BallStrikeNum',\n",
    "# 'GroundTruth']\n",
    "class PitchingDataset(Dataset): #TODO set this up to work for our dataset\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "    #defining prepprocessing functions\n",
    "\n",
    "    #preprocessing function used to calculate the plate location and isStrike classifier (this will be different than PitchCall (even in terms of BallCalled vs StrikeCalled))\n",
    "    #returns tuple: classification, isStrike\n",
    "    #classification: heart (strike) = 0, shadow (strike) = 1, shadow (ball) = 2, chase (ball) = 3, waste (ball) = 4\n",
    "    def PlateZone(self, PlateLocHeight, PlateLocSide):\n",
    "      foot = 12\n",
    "\n",
    "      #waste outside 84in to 6in, -20in to 20in horizontal, (strike zone * 200%)\n",
    "      if ((PlateLocHeight > 7 or PlateLocHeight < 0.5) and (PlateLocSide < -(20/foot) or PlateLocSide > (20/foot))):\n",
    "        return 4\n",
    "\n",
    "      #heart inside 38in to 22in vertical, -6.7in to 6.7in horizontal, (strike zone size * 67%)\n",
    "      if ((PlateLocHeight < (38/foot) and PlateLocHeight > (22/foot)) and (PlateLocSide > (-6.7/foot) and PlateLocSide < (6.7/foot))):\n",
    "        return 0\n",
    "\n",
    "      #strike zone inside 42in to 18in vertical, -10in to 10in horizontal\n",
    "      if (PlateLocHeight < (42/foot) and PlateLocHeight > (18/foot) and (PlateLocSide > (-10/foot) and PlateLocSide < (10/foot))):\n",
    "        return 1\n",
    "\n",
    "      #shadow inside 46in to 14in vertical, -13.3in to 13.3in horizontal, (strike zone size * 133%)\n",
    "      if (PlateLocHeight < (46/foot) and PlateLocHeight > (14/foot) and (PlateLocSide > (-13.3/foot) and PlateLocSide < (13.3/foot))):\n",
    "        return 2\n",
    "\n",
    "      #chase inside 84in to 6in, -20in to 20in horizontal, (strike zone * 200%)\n",
    "      return 3\n",
    "\n",
    "    #preprocessing function used to generate a single number that will be used to classify the ball/strike count before the current pitch \n",
    "    #returns int [0 - 11]\n",
    "    def PitchCount(self, balls, strikes):\n",
    "    # Strikes: 0  1   2\n",
    "    # Balls v|---------- \n",
    "    #       0| 0  1   2 \n",
    "    #       1| 3  4   5\n",
    "    #       2| 6  7   8\n",
    "    #       3| 9  10  11\n",
    "      if(balls == 0):\n",
    "        if(strikes == 0):\n",
    "          return 0\n",
    "        if (strikes == 1):\n",
    "          return 1\n",
    "        return 2\n",
    "      if(balls == 1):\n",
    "        if(strikes == 0):\n",
    "          return 3\n",
    "        if (strikes == 1):\n",
    "          return 4\n",
    "        return 5\n",
    "      if(balls == 2):\n",
    "        if(strikes == 0):\n",
    "          return 6\n",
    "        if (strikes == 1):\n",
    "          return 7\n",
    "        return 8\n",
    "      if(balls == 3):\n",
    "        if(strikes == 0):\n",
    "          return 9\n",
    "        if (strikes == 1):\n",
    "          return 10\n",
    "        return 11\n",
    "\n",
    "    #this preprocessing function generates the ground truth hitability of a pitch\n",
    "    #these values will definitely need to be adjusted and should probably be hyperparameters\n",
    "    def GenerateGroundTruthLabels(self, pitchCall):\n",
    "      if pitchCall == 'BallCalled':\n",
    "        return -1\n",
    "      if pitchCall == 'BallIntentional' or pitchCall == 'HitByPitch':\n",
    "        return 0\n",
    "      if pitchCall == 'StrikeSwinging' or pitchCall == 'StrikeCalled':\n",
    "        return 1\n",
    "      else:\n",
    "        return 2\n",
    "\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "      #TODO \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with data.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #import the columns we will need for training and preprocessing\n",
    "        self.root_dir = root_dir\n",
    "        self.bball_data = pd.read_csv(self.root_dir+csv_file, usecols=['Pitcher', 'PitcherThrows', 'Batter', 'BatterSide', 'PitchCall', 'KorBB', 'PlayResult', 'RunsScored', 'VertBreak', 'HorzBreak', 'ZoneSpeed', 'VertApprAngle', 'HorzApprAngle', 'ZoneTime', 'PlateLocHeight', 'PlateLocSide', 'Balls', 'Strikes', 'TaggedPitchType'])\n",
    "        '''\n",
    "        #run preprocessing functions\n",
    "        self.bball_data['Zone'] = self.bball_data.apply(lambda pitch : self.PlateZone(pitch['PlateLocHeight'], pitch['PlateLocSide']), axis=1)\n",
    "        self.bball_data['BallStrikeNum'] = self.bball_data.apply(lambda pitch : self.PitchCount(pitch['Balls'], pitch['Strikes']), axis=1)\n",
    "        \n",
    "\n",
    "        #drop all features we no longer need (ones only used for preprocessing)\n",
    "        self.bball_data.drop(labels=['PlateLocHeight', 'PlateLocSide', 'Balls', 'Strikes'], axis=1)\n",
    "\n",
    "        #\n",
    "        categorical_columns = ['Pitcher','PitcherThrows', 'Batter', 'BatterSide', 'PitchCall', 'KorBB', 'PlayResult', 'Zone', 'BallStrikeNum', 'TaggedPitchType']\n",
    "\n",
    "        for category in categorical_columns:\n",
    "          self.bball_data[category] = self.bball_data[category].astype('category')\n",
    "        '''\n",
    "\n",
    "        #TODO normalize data\n",
    "\n",
    "        #generate ground truth labels\n",
    "        self.bball_data['GroundTruth'] = self.bball_data.apply(lambda pitch : self.GenerateGroundTruthLabels(pitch['PitchCall']), axis=1)\n",
    "\n",
    "        feature_columns = self.bball_data[['VertBreak', 'HorzBreak', 'ZoneSpeed', 'VertApprAngle', 'HorzApprAngle', 'ZoneTime', 'PlateLocHeight', 'PlateLocSide']].copy()\n",
    "        target_column = self.bball_data[['GroundTruth']].copy\n",
    "\n",
    "        #build the tensor of data\n",
    "        data_tensor = torch.from_numpy(feature_columns.to_numpy())\n",
    "        #Tensor has 8 features\n",
    "        print(feature_columns.iloc[0])\n",
    "        print(feature_columns.to_numpy()[0,:])\n",
    "        print(data_tensor[0,:])\n",
    "        \n",
    "        #build the tensor of targets\n",
    "        target_tensor = torch.from_numpy(target_column.to_numpy())\n",
    "\n",
    "        #print the labels of the features we will use to train our network\n",
    "        print(self.bball_data.keys())\n",
    "\n",
    "        #splitting data into training, validation, testing\n",
    "        total_samples = len(self.bball_data.index)\n",
    "        training_samples = math.floor(0.6*total_samples)\n",
    "        validation_samples = math.ceil(0.2*total_samples)\n",
    "        testing_samples = math.ceil(0.2*total_samples)\n",
    "\n",
    "        sum = training_samples+testing_samples+validation_samples\n",
    "\n",
    "        print(\"total samples:\",total_samples,\n",
    "              \"\\ntraining samples:\",training_samples,\n",
    "              \"\\nvalidation samples:\",validation_samples,\n",
    "              \"\\ntesting samples:\",testing_samples,\n",
    "              \"\\nsum of training, validation, and test samples:\",sum)\n",
    "\n",
    "        #makes shuffled version of the data\n",
    "        indices = np.arange(total_samples)\n",
    "        np.random.shuffle(indices) #TODO get ground truth for data sets using these indices\n",
    "        shuffled_bball_data = self.bball_data.reindex(indices).reset_index()\n",
    "\n",
    "        #gets the amount of random data points as determined by set proportion\n",
    "        self.training_data = shuffled_bball_data.iloc[0:training_samples]\n",
    "        self.validation_data = shuffled_bball_data.iloc[training_samples:training_samples+validation_samples]\n",
    "        self.testing_data = shuffled_bball_data.iloc[training_samples+validation_samples:training_samples+validation_samples+testing_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bball_data)\n",
    "\n",
    "    def __getitem__(self, idx): #TODO make this get the next data point\n",
    "        #https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "        '''\n",
    "        if isValidation:\n",
    "          curr_data = self.validation_data\n",
    "        else if isTesting:\n",
    "          curr_data = self.testing_data\n",
    "        else:\n",
    "          curr_data = self.training_data\n",
    "        '''\n",
    "        num_data = len(self.bball_data)\n",
    "        num_features = len(self.bball_data.columns) - 2 #one to get proper index, one for the target\n",
    "        ground_truth_index = num_features + 1\n",
    "\n",
    "        #print(num_data)\n",
    "        #print(num_features)\n",
    "        #print(ground_truth_index)\n",
    "\n",
    "        all_data = self.bball_data.iloc[[idx]]\n",
    "\n",
    "        #print(all_data)\n",
    "        #print(all_data.shape)\n",
    "\n",
    "        ground_truth = all_data.pop('GroundTruth')\n",
    "        predictive_data = all_data\n",
    "\n",
    "        #print(ground_truth)\n",
    "        #print(predictive_data)\n",
    "\n",
    "        sample = {'predictive_data': predictive_data, 'ground_truth': ground_truth}\n",
    "\n",
    "        #print(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    #will return a dataframe containing all rows which have the specified key value in the feature column\n",
    "    def getAllWithFeature(self, feature, key):\n",
    "        possible_features = ['Pitcher', 'PitcherThrows', 'Batter', 'BatterSide', 'TaggedPitchType']\n",
    "\n",
    "        if feature not in possible_features:\n",
    "          return None\n",
    "        \n",
    "        pitches = self.bball_data.loc[self.bball_data[feature] == key]\n",
    "\n",
    "        return pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VertBreak        -14.967383\n",
      "HorzBreak         11.033162\n",
      "ZoneSpeed         84.297505\n",
      "VertApprAngle     -4.245969\n",
      "HorzApprAngle     -1.617909\n",
      "ZoneTime           0.403931\n",
      "PlateLocHeight     3.528178\n",
      "PlateLocSide       0.015015\n",
      "Name: 0, dtype: float64\n",
      "[-1.49673834e+01  1.10331617e+01  8.42975051e+01 -4.24596924e+00\n",
      " -1.61790877e+00  4.03930541e-01  3.52817813e+00  1.50152510e-02]\n",
      "tensor([-1.4967e+01,  1.1033e+01,  8.4298e+01, -4.2460e+00, -1.6179e+00,\n",
      "         4.0393e-01,  3.5282e+00,  1.5015e-02], dtype=torch.float64)\n",
      "Index(['Pitcher', 'PitcherThrows', 'Batter', 'BatterSide', 'Balls', 'Strikes',\n",
      "       'TaggedPitchType', 'PitchCall', 'KorBB', 'PlayResult', 'RunsScored',\n",
      "       'VertBreak', 'HorzBreak', 'PlateLocHeight', 'PlateLocSide', 'ZoneSpeed',\n",
      "       'VertApprAngle', 'HorzApprAngle', 'ZoneTime', 'GroundTruth'],\n",
      "      dtype='object')\n",
      "total samples: 1040373 \n",
      "training samples: 624223 \n",
      "validation samples: 208075 \n",
      "testing samples: 208075 \n",
      "sum of training, validation, and test samples: 1040373\n",
      "648037\n"
     ]
    }
   ],
   "source": [
    "dataset = PitchingDataset(bball_data_2_file,data_folder)\n",
    "train_data = dataset.training_data\n",
    "validation_data = dataset.validation_data\n",
    "test_data = dataset.testing_data\n",
    "ballsCalled = dataset.getAllWithFeature('BatterSide', 'Right')\n",
    "\n",
    "print(len(ballsCalled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an implementation of the soft decision tree model\n",
    "#https://github.com/kimhc6028/soft-decision-tree/blob/master/model.py\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class InnerNode():\n",
    "\n",
    "    def __init__(self, depth, args):\n",
    "        self.args = args\n",
    "        self.fc = nn.Linear(self.args.input_dim, 1)\n",
    "        beta = torch.randn(1)\n",
    "        #beta = beta.expand((self.args.batch_size, 1))\n",
    "        if self.args.cuda:\n",
    "            beta = beta.cuda()\n",
    "        self.beta = nn.Parameter(beta)\n",
    "        self.leaf = False\n",
    "        self.prob = None\n",
    "        self.leaf_accumulator = []\n",
    "        self.lmbda = self.args.lmbda * 2 ** (-depth)\n",
    "        self.build_child(depth)\n",
    "        self.penalties = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.leaf_accumulator = []\n",
    "        self.penalties = []\n",
    "        self.left.reset()\n",
    "        self.right.reset()\n",
    "\n",
    "    def build_child(self, depth):\n",
    "        if depth < self.args.max_depth:\n",
    "            self.left = InnerNode(depth+1, self.args)\n",
    "            self.right = InnerNode(depth+1, self.args)\n",
    "        else :\n",
    "            self.left = LeafNode(self.args)\n",
    "            self.right = LeafNode(self.args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(F.sigmoid(self.beta*self.fc(x)))\n",
    "    \n",
    "    def select_next(self, x):\n",
    "        prob = self.forward(x)\n",
    "        if prob < 0.5:\n",
    "            return(self.left, prob)\n",
    "        else:\n",
    "            return(self.right, prob)\n",
    "\n",
    "    def cal_prob(self, x, path_prob):\n",
    "        self.prob = self.forward(x) #probability of selecting right node\n",
    "        self.path_prob = path_prob\n",
    "        left_leaf_accumulator = self.left.cal_prob(x, path_prob * (1-self.prob))\n",
    "        right_leaf_accumulator = self.right.cal_prob(x, path_prob * self.prob)\n",
    "        self.leaf_accumulator.extend(left_leaf_accumulator)\n",
    "        self.leaf_accumulator.extend(right_leaf_accumulator)\n",
    "        return(self.leaf_accumulator)\n",
    "\n",
    "    def get_penalty(self):\n",
    "        penalty = (torch.sum(self.prob * self.path_prob) / torch.sum(self.path_prob), self.lmbda)\n",
    "        if not self.left.leaf:\n",
    "            left_penalty = self.left.get_penalty()\n",
    "            right_penalty = self.right.get_penalty()\n",
    "            self.penalties.append(penalty)\n",
    "            self.penalties.extend(left_penalty)\n",
    "            self.penalties.extend(right_penalty)\n",
    "        return(self.penalties)\n",
    "\n",
    "\n",
    "class LeafNode():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.param = torch.randn(self.args.output_dim)\n",
    "        if self.args.cuda:\n",
    "            self.param = self.param.cuda()\n",
    "        self.param = nn.Parameter(self.param)\n",
    "        self.leaf = True\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self):\n",
    "        return(self.softmax(self.param.view(1,-1)))\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def cal_prob(self, x, path_prob):\n",
    "        Q = self.forward()\n",
    "        #Q = Q.expand((self.args.batch_size, self.args.output_dim))\n",
    "        Q = Q.expand((path_prob.size()[0], self.args.output_dim))\n",
    "        return([[path_prob, Q]])\n",
    "\n",
    "\n",
    "class SoftDecisionTree(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(SoftDecisionTree, self).__init__()\n",
    "        self.args = args\n",
    "        self.root = InnerNode(1, self.args)\n",
    "        self.collect_parameters() ##collect parameters and modules under root node\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=self.args.lr, momentum=self.args.momentum)\n",
    "        self.test_acc = []\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        self.best_accuracy = 0.0\n",
    "\n",
    "    def define_extras(self, batch_size):\n",
    "        ##define target_onehot and path_prob_init batch size, because these need to be defined according to batch size, which can be differ\n",
    "        self.target_onehot = torch.FloatTensor(batch_size, self.args.output_dim)\n",
    "        self.target_onehot = Variable(self.target_onehot)\n",
    "        self.path_prob_init = Variable(torch.ones(batch_size, 1))\n",
    "        if self.args.cuda:\n",
    "            self.target_onehot = self.target_onehot.cuda()\n",
    "            self.path_prob_init = self.path_prob_init.cuda()\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        node = self.root\n",
    "        path_prob = Variable(torch.ones(self.args.batch_size, 1))\n",
    "        while not node.leaf:\n",
    "            node, prob = node.select_next(x)\n",
    "            path_prob *= prob\n",
    "        return node()\n",
    "    '''        \n",
    "    def cal_loss(self, x, y):\n",
    "        batch_size = y.size()[0]\n",
    "        leaf_accumulator = self.root.cal_prob(x, self.path_prob_init)\n",
    "        loss = 0.\n",
    "        max_prob = [-1. for _ in range(batch_size)]\n",
    "        max_Q = [torch.zeros(self.args.output_dim) for _ in range(batch_size)]\n",
    "        for (path_prob, Q) in leaf_accumulator:\n",
    "            TQ = torch.bmm(y.view(batch_size, 1, self.args.output_dim), torch.log(Q).view(batch_size, self.args.output_dim, 1)).view(-1,1)\n",
    "            loss += path_prob * TQ\n",
    "            path_prob_numpy = path_prob.cpu().data.numpy().reshape(-1)\n",
    "            for i in range(batch_size):\n",
    "                if max_prob[i] < path_prob_numpy[i]:\n",
    "                    max_prob[i] = path_prob_numpy[i]\n",
    "                    max_Q[i] = Q[i]\n",
    "        loss = loss.mean()\n",
    "        penalties = self.root.get_penalty()\n",
    "        C = 0.\n",
    "        for (penalty, lmbda) in penalties:\n",
    "            C -= lmbda * 0.5 *(torch.log(penalty) + torch.log(1-penalty))\n",
    "        output = torch.stack(max_Q)\n",
    "        self.root.reset() ##reset all stacked calculation\n",
    "        return(-loss + C, output) ## -log(loss) will always output non, because loss is always below zero. I suspect this is the mistake of the paper?\n",
    "\n",
    "    def collect_parameters(self):\n",
    "        nodes = [self.root]\n",
    "        self.module_list = nn.ModuleList()\n",
    "        self.param_list = nn.ParameterList()\n",
    "        while nodes:\n",
    "            node = nodes.pop(0)\n",
    "            if node.leaf:\n",
    "                param = node.param\n",
    "                self.param_list.append(param)\n",
    "            else:\n",
    "                fc = node.fc\n",
    "                beta = node.beta\n",
    "                nodes.append(node.right)\n",
    "                nodes.append(node.left)\n",
    "                self.param_list.append(beta)\n",
    "                self.module_list.append(fc)\n",
    "\n",
    "    def train_(self, train_loader, epoch):\n",
    "        self.train()\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            correct = 0\n",
    "            if self.args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            #data = data.view(self.args.batch_size,-1)\n",
    "            target = Variable(target)\n",
    "            target_ = target.view(-1,1)\n",
    "            batch_size = target_.size()[0]\n",
    "            data = data.view(batch_size,-1)\n",
    "            ##convert int target to one-hot vector\n",
    "            data = Variable(data)\n",
    "            if not batch_size == self.args.batch_size: #because we have to initialize parameters for batch_size, tensor not matches with batch size cannot be trained\n",
    "                self.define_extras(batch_size)\n",
    "            self.target_onehot.data.zero_()            \n",
    "            self.target_onehot.scatter_(1, target_, 1.)\n",
    "            self.optimizer.zero_grad()\n",
    "            print(data.size())\n",
    "            loss, output = self.cal_loss(data, self.target_onehot)\n",
    "            #loss.backward(retain_variables=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "            accuracy = 100. * correct / len(data)\n",
    "\n",
    "            if batch_idx % self.args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.4f}%)'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(),\n",
    "                    correct, len(data),\n",
    "                    accuracy))\n",
    "\n",
    "    def test_(self, test_loader, epoch):\n",
    "        self.eval()\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            if self.args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            target = Variable(target)\n",
    "            target_ = target.view(-1,1)\n",
    "            batch_size = target_.size()[0]\n",
    "            data = data.view(batch_size,-1)\n",
    "            ##convert int target to one-hot vector\n",
    "            data = Variable(data)\n",
    "            if not batch_size == self.args.batch_size: #because we have to initialize parameters for batch_size, tensor not matches with batch size cannot be trained\n",
    "                self.define_extras(batch_size)\n",
    "            self.target_onehot.data.zero_()            \n",
    "            self.target_onehot.scatter_(1, target_, 1.)\n",
    "            _, output = self.cal_loss(data, self.target_onehot)\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print('\\nTest set: Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "            correct, len(test_loader.dataset),\n",
    "            accuracy))\n",
    "        self.test_acc.append(accuracy)\n",
    "\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.save_best('./result')\n",
    "            self.best_accuracy = accuracy\n",
    "\n",
    "    def save_best(self, path):\n",
    "        try:\n",
    "            os.makedirs('./result')\n",
    "        except:\n",
    "            print('directory ./result already exists')\n",
    "\n",
    "        with open(os.path.join(path, 'best_model.pkl'), 'wb') as output_file:\n",
    "            pickle.dump(self, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "265564",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 265564",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IANMCD~1\\AppData\\Local\\Temp/ipykernel_16852/156515514.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\IANMCD~1\\AppData\\Local\\Temp/ipykernel_16852/1506692128.py\u001b[0m in \u001b[0;36mtrain_\u001b[1;34m(self, train_loader, epoch)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_extras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 265564"
     ]
    }
   ],
   "source": [
    "#This is driver code used to train the soft decision tree model\n",
    "#https://github.com/kimhc6028/soft-decision-tree/blob/master/main.py\n",
    "from __future__ import print_function\n",
    "import sys,os\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "sys.argv = ['']\n",
    "\n",
    "#training_data = <-- define to get rid of name error\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--input-dim', type=int, default=len(train_data.columns), metavar='N', #set to number of columns we are using\n",
    "                    help='input dimension size(default: 28 * 28)')\n",
    "parser.add_argument('--output-dim', type=int, default=10, metavar='N',\n",
    "                    help='output dimension size(default: 10)')\n",
    "parser.add_argument('--max-depth', type=int, default=8, metavar='N',\n",
    "                    help='maximum depth of tree(default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "                    help='number of epochs to train (default: 40)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lmbda', type=float, default=0.1, metavar='LR',\n",
    "                    help='temperature rate (default: 0.1)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# validation_loader = torch.utils.data.DataLoader(\n",
    "#     PitchingDataset(bball_data_2_file,data_folder,train=False,validation=True),\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('./data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#     ])),\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "def save_result(acc):\n",
    "    try:\n",
    "        os.makedirs('./result')\n",
    "    except:\n",
    "        print('directory ./result already exists')\n",
    "    filename = os.path.join('./result/', 'bp.pickle')\n",
    "    f = open(filename,'wb')\n",
    "    pickle.dump(acc, f)\n",
    "    f.close()\n",
    "\n",
    "model = SoftDecisionTree(args)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model.train_(train_loader, epoch)\n",
    "    model.test_(test_loader, epoch)\n",
    "\n",
    "save_result(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e50f20eadda11b8291276ce54281c99b863cb363d1e81d1cfeaec61992551ca8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('MLSclass': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
